{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/p/fast-ai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    lock\n",
    "except:\n",
    "    %cd ..\n",
    "    CUR_DIR = os.getcwd()\n",
    "    lock = 'Locked'\n",
    "CUR_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aims\n",
    "1. Train linear model on VGG16 output to predict cat vs dogs\n",
    "2. Replace final dense layer of VGG16 to predict\n",
    "3. Fine-Train dense VGG16 layers\n",
    "4. Fine-Train whole of VGG16 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs/\n",
      "/home/ubuntu/p/fast-ai/data/cats_dogs/sample/\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = CUR_DIR + '/data/cats_dogs/'; print(DATA_PATH)\n",
    "SAMPLE_PATH = DATA_PATH + 'sample/'; print(SAMPLE_PATH)\n",
    "SPLIT_PATHS = ['train/','val/','test/']\n",
    "CREATE_DATA_SETS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and setup of data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup kaggle-cli config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kg config -u \"dbcerigo\" -c \"dogs-vs-cats-redux-kernels-edition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.mkdir(DATA_PATH)\n",
    "    %cd {DATA_PATH}\n",
    "    !kg download\n",
    "    !unzip -qn test.zip    \n",
    "    !unzip -qn train.zip\n",
    "    os.mkdir(DATA_PATH+'val')\n",
    "if not os.path.isdir(SAMPLE_PATH):\n",
    "    os.mkdir(SAMPLE_PATH)\n",
    "    for path in SPLIT_PATHS:\n",
    "        os.mkdir(SAMPLE_PATH+SAMPLE_PATH+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'test': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%cd {DATA_PATH}\n",
    "# test size\n",
    "!ls -1 train | wc -l # post creating data should output 2 (cats, dogs)\n",
    "# train size\n",
    "!ls -1 test | wc -l # post creating data should output 1 (unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\tsample_submission.csv  test  train  val\n",
      "test  train  val\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}\n",
    "!ls sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if CREATE_DATA_SETS: \n",
    "    !rm  {DATA_PATH}test.zip\n",
    "    !rm  {DATA_PATH}train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST, seperate off a val set (do first to avoid leaks etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs/train\n"
     ]
    }
   ],
   "source": [
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_paths = glob('*.jpg')\n",
    "im_paths[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_paths = np.random.permutation(im_paths)\n",
    "if CREATE_DATA_SETS: \n",
    "    for i in range(2000): os.rename(im_paths[i], DATA_PATH+'val/'+im_paths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Move sample parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs/train\n"
     ]
    }
   ],
   "source": [
    "%cd {DATA_PATH}train\n",
    "im_paths = glob('*.jpg') #get new paths list as don't want to ones that are in val\n",
    "im_paths = np.random.permutation(im_paths)\n",
    "if CREATE_DATA_SETS:\n",
    "    for i in range(100): shutil.copyfile(im_paths[i], SAMPLE_PATH+'train/'+im_paths[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_paths = np.random.permutation(im_paths)\n",
    "if CREATE_DATA_SETS:\n",
    "    for i in range(10): shutil.copyfile(im_paths[i], SAMPLE_PATH+'val/'+im_paths[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs/test\n",
      "/home/ubuntu/p/fast-ai/data/cats_dogs\n"
     ]
    }
   ],
   "source": [
    "%cd {DATA_PATH}test\n",
    "im_paths = glob('*.jpg') \n",
    "im_paths = np.random.permutation(im_paths)\n",
    "if CREATE_DATA_SETS:\n",
    "    for i in range(20): shutil.copyfile(im_paths[i], SAMPLE_PATH+'test/'+im_paths[i]) \n",
    "%cd {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs\n",
      "1\n",
      "2\n",
      "sample:\n",
      "test  train  val\n",
      "\n",
      "sample/test:\n",
      "unknown\n",
      "\n",
      "sample/test/unknown:\n",
      "1387.jpg  2586.jpg  3801.jpg  4913.jpg\t6195.jpg  8152.jpg  9078.jpg\n",
      "1633.jpg  272.jpg   4402.jpg  5071.jpg\t6778.jpg  8723.jpg  9290.jpg\n",
      "1940.jpg  3180.jpg  4559.jpg  5241.jpg\t7691.jpg  8856.jpg\n",
      "\n",
      "sample/train:\n",
      "cats  dogs\n",
      "\n",
      "sample/train/cats:\n",
      "cat.10506.jpg  cat.1852.jpg  cat.3930.jpg  cat.6134.jpg  cat.833.jpg\n",
      "cat.10827.jpg  cat.2032.jpg  cat.4018.jpg  cat.6267.jpg  cat.8531.jpg\n",
      "cat.11293.jpg  cat.2071.jpg  cat.4108.jpg  cat.633.jpg\t cat.8691.jpg\n",
      "cat.1138.jpg   cat.2102.jpg  cat.4410.jpg  cat.6383.jpg  cat.9080.jpg\n",
      "cat.11511.jpg  cat.219.jpg   cat.4459.jpg  cat.6420.jpg  cat.9152.jpg\n",
      "cat.11559.jpg  cat.221.jpg   cat.4929.jpg  cat.7076.jpg  cat.9368.jpg\n",
      "cat.1171.jpg   cat.247.jpg   cat.5107.jpg  cat.7203.jpg  cat.9404.jpg\n",
      "cat.11733.jpg  cat.3067.jpg  cat.5122.jpg  cat.7205.jpg  cat.9750.jpg\n",
      "cat.12041.jpg  cat.3120.jpg  cat.5277.jpg  cat.8216.jpg  cat.9800.jpg\n",
      "cat.12131.jpg  cat.3280.jpg  cat.5356.jpg  cat.8288.jpg  cat.9952.jpg\n",
      "cat.1754.jpg   cat.3800.jpg  cat.5932.jpg  cat.8294.jpg\n",
      "\n",
      "sample/train/dogs:\n",
      "dog.10039.jpg  dog.1383.jpg  dog.2594.jpg  dog.6013.jpg  dog.7838.jpg\n",
      "dog.10063.jpg  dog.1448.jpg  dog.283.jpg   dog.6092.jpg  dog.7999.jpg\n",
      "dog.10748.jpg  dog.1526.jpg  dog.2870.jpg  dog.6221.jpg  dog.8097.jpg\n",
      "dog.10873.jpg  dog.1875.jpg  dog.3560.jpg  dog.6327.jpg  dog.8223.jpg\n",
      "dog.11194.jpg  dog.1987.jpg  dog.3686.jpg  dog.643.jpg\t dog.8739.jpg\n",
      "dog.11286.jpg  dog.2025.jpg  dog.3995.jpg  dog.7004.jpg  dog.9880.jpg\n",
      "dog.11300.jpg  dog.2162.jpg  dog.4807.jpg  dog.7038.jpg\n",
      "dog.11417.jpg  dog.2168.jpg  dog.4847.jpg  dog.7298.jpg\n",
      "dog.1158.jpg   dog.2245.jpg  dog.5111.jpg  dog.7323.jpg\n",
      "dog.11662.jpg  dog.2368.jpg  dog.5296.jpg  dog.7569.jpg\n",
      "\n",
      "sample/val:\n",
      "cats  dogs\n",
      "\n",
      "sample/val/cats:\n",
      "cat.11150.jpg  cat.1377.jpg  cat.4688.jpg  cat.6231.jpg\n",
      "\n",
      "sample/val/dogs:\n",
      "dog.1859.jpg  dog.5314.jpg  dog.8880.jpg\n",
      "dog.3236.jpg  dog.5391.jpg  dog.9344.jpg\n"
     ]
    }
   ],
   "source": [
    "%cd {DATA_PATH}\n",
    "# test size\n",
    "!ls -1 test | wc -l\n",
    "# train size\n",
    "!ls -1 train | wc -l\n",
    "!ls -R sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move_class_files(path):\n",
    "    targer_dir =path.split('/')[-2]\n",
    "    assert targer_dir == 'train' or targer_dir == 'val'\n",
    "    %cd {path}\n",
    "    %mkdir cats\n",
    "    %mkdir dogs\n",
    "    %mv cat.*.jpg cats/\n",
    "    %mv dog.*.jpg dogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move_test_files(path):\n",
    "    assert path.split('/')[-2] == 'test'\n",
    "    %cd {path}\n",
    "    %mkdir unknown\n",
    "    %mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_DATA_SETS:\n",
    "    make_move_class_files(SAMPLE_PATH+'train/')\n",
    "    make_move_class_files(SAMPLE_PATH+'val/')\n",
    "    make_move_test_files(SAMPLE_PATH+'test/')\n",
    "    make_move_class_files(DATA_PATH+'train/')\n",
    "    make_move_class_files(DATA_PATH+'val/')\n",
    "    make_move_test_files(DATA_PATH+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/p/fast-ai/data/cats_dogs\n",
      "12500\n",
      "11507\n",
      "11493\n",
      "sample:\n",
      "test  train  val\n",
      "\n",
      "sample/test:\n",
      "unknown\n",
      "\n",
      "sample/test/unknown:\n",
      "1387.jpg  2586.jpg  3801.jpg  4913.jpg\t6195.jpg  8152.jpg  9078.jpg\n",
      "1633.jpg  272.jpg   4402.jpg  5071.jpg\t6778.jpg  8723.jpg  9290.jpg\n",
      "1940.jpg  3180.jpg  4559.jpg  5241.jpg\t7691.jpg  8856.jpg\n",
      "\n",
      "sample/train:\n",
      "cats  dogs\n",
      "\n",
      "sample/train/cats:\n",
      "cat.10506.jpg  cat.1852.jpg  cat.3930.jpg  cat.6134.jpg  cat.833.jpg\n",
      "cat.10827.jpg  cat.2032.jpg  cat.4018.jpg  cat.6267.jpg  cat.8531.jpg\n",
      "cat.11293.jpg  cat.2071.jpg  cat.4108.jpg  cat.633.jpg\t cat.8691.jpg\n",
      "cat.1138.jpg   cat.2102.jpg  cat.4410.jpg  cat.6383.jpg  cat.9080.jpg\n",
      "cat.11511.jpg  cat.219.jpg   cat.4459.jpg  cat.6420.jpg  cat.9152.jpg\n",
      "cat.11559.jpg  cat.221.jpg   cat.4929.jpg  cat.7076.jpg  cat.9368.jpg\n",
      "cat.1171.jpg   cat.247.jpg   cat.5107.jpg  cat.7203.jpg  cat.9404.jpg\n",
      "cat.11733.jpg  cat.3067.jpg  cat.5122.jpg  cat.7205.jpg  cat.9750.jpg\n",
      "cat.12041.jpg  cat.3120.jpg  cat.5277.jpg  cat.8216.jpg  cat.9800.jpg\n",
      "cat.12131.jpg  cat.3280.jpg  cat.5356.jpg  cat.8288.jpg  cat.9952.jpg\n",
      "cat.1754.jpg   cat.3800.jpg  cat.5932.jpg  cat.8294.jpg\n",
      "\n",
      "sample/train/dogs:\n",
      "dog.10039.jpg  dog.1383.jpg  dog.2594.jpg  dog.6013.jpg  dog.7838.jpg\n",
      "dog.10063.jpg  dog.1448.jpg  dog.283.jpg   dog.6092.jpg  dog.7999.jpg\n",
      "dog.10748.jpg  dog.1526.jpg  dog.2870.jpg  dog.6221.jpg  dog.8097.jpg\n",
      "dog.10873.jpg  dog.1875.jpg  dog.3560.jpg  dog.6327.jpg  dog.8223.jpg\n",
      "dog.11194.jpg  dog.1987.jpg  dog.3686.jpg  dog.643.jpg\t dog.8739.jpg\n",
      "dog.11286.jpg  dog.2025.jpg  dog.3995.jpg  dog.7004.jpg  dog.9880.jpg\n",
      "dog.11300.jpg  dog.2162.jpg  dog.4807.jpg  dog.7038.jpg\n",
      "dog.11417.jpg  dog.2168.jpg  dog.4847.jpg  dog.7298.jpg\n",
      "dog.1158.jpg   dog.2245.jpg  dog.5111.jpg  dog.7323.jpg\n",
      "dog.11662.jpg  dog.2368.jpg  dog.5296.jpg  dog.7569.jpg\n",
      "\n",
      "sample/val:\n",
      "cats  dogs\n",
      "\n",
      "sample/val/cats:\n",
      "cat.11150.jpg  cat.1377.jpg  cat.4688.jpg  cat.6231.jpg\n",
      "\n",
      "sample/val/dogs:\n",
      "dog.1859.jpg  dog.5314.jpg  dog.8880.jpg\n",
      "dog.3236.jpg  dog.5391.jpg  dog.9344.jpg\n"
     ]
    }
   ],
   "source": [
    "%cd {DATA_PATH}\n",
    "# test size\n",
    "!ls -1 test/unknown | wc -l\n",
    "# train size\n",
    "!ls -1 train/cats | wc -l\n",
    "!ls -1 train/dogs | wc -l\n",
    "!ls -R sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train linear model on VGG16 output to predict cat vs dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Keras model weights\n",
    "\n",
    "* Model weights can be considered seperate from model\n",
    "* Model is just a DAG, a set of operations (or flow) on a tensor, where the actual values are not yet defined - these are the weights (the types and shapes etc. are defined)\n",
    "* Keras uses `keras.utils.data_utils.get_file` to do lazy downloading and caching of files (such as model weights)\n",
    "    - Default cache locations is `~/.keras/models/`, but better for us to have it in EBS so don't have to redownload each time\n",
    "  \n",
    "  \n",
    "* Good overview of keras Model api https://keras.io/models/about-keras-models/ including `model.set_weights(weights)`\n",
    "* Includes good stuff on saving and loading weights https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download weights for VGG16 but cache to our models dir\n",
    "MODEL_PATH = CUR_DIR + '/model/'\n",
    "KERAS_MODEL_URL = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/'\n",
    "FULL_VGG_WEIGHTS_NAME = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "NOTOP_VGG_WEIGHTS_NAME = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    }
   ],
   "source": [
    "full_weights_path = get_file(FULL_VGG_WEIGHTS_NAME, KERAS_MODEL_URL+FULL_VGG_WEIGHTS_NAME, cache_subdir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    }
   ],
   "source": [
    "notop_weights_path = get_file(NOTOP_VGG_WEIGHTS_NAME, KERAS_MODEL_URL+NOTOP_VGG_WEIGHTS_NAME, cache_subdir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need bigger instance from here so hold model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = keras.applications.vgg16.VGG16(include_top=True, weights=None,\n",
    "                               input_tensor=None, input_shape=None,\n",
    "                               pooling=None, classes=1000)\n",
    "#read the source code of stuff if it seems mysterious to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544.0\n",
      "Trainable params: 138,357,544.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use image net weights\n",
    "vgg.load_weights(full_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Training Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_gen = ImageDataGenerator()\n",
    "# can add a ton of params here to add automated augmentation to the images it feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/preprocessing/image/\n",
    "* check my sneaky `nb_batches` which I can use for setting number of steps per epoch\n",
    "# NOTE:\n",
    "* Think that adding shuffle shifts the data order but not the labels?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = img_gen.flow_from_directory(DATA_PATH+'train/',\n",
    "                                      target_size= (224,224), # this should match the input layer size\n",
    "                                      color_mode='rgb', # alternative is 'grayscale'\n",
    "                                      classes=None, # will infere them from directory structure\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=200, # default is 32\n",
    "                                      shuffle=False,\n",
    "                                      seed=2017 # for reproducing\n",
    "                                     )\n",
    "train_gen.nb_batches = int(ceil(train_gen.samples / train_gen.batch_size))\n",
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_gen)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_gen = img_gen.flow_from_directory(DATA_PATH+'val/',\n",
    "                                      target_size= (224,224), # this should match the input layer size\n",
    "                                      color_mode='rgb', # alternative is 'grayscale'\n",
    "                                      classes=None, # will infere them from directory structure\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=124, # default is 32\n",
    "                                      shuffle=False,\n",
    "                                      seed=2017 # for reproducing\n",
    "                                     )\n",
    "val_gen.nb_batches = int(ceil(val_gen.samples / val_gen.batch_size))\n",
    "val_gen.nb_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking CPU/GPU usage\n",
    "* check CPU usage `htop`\n",
    "* check GPU usage `watch -n1 nvidia-smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13193129690101296304\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 292945920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8828759571952064255\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# get devices tensorflow sees\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.47573770e-09,   2.62655830e-09,   2.23767649e-09, ...,\n",
       "          1.18900306e-10,   7.28344105e-07,   5.36811240e-05],\n",
       "       [  6.83440078e-08,   2.40387209e-07,   2.09702739e-06, ...,\n",
       "          2.61103494e-08,   1.01854812e-05,   1.26359714e-02],\n",
       "       [  7.72301778e-09,   1.17497791e-06,   2.33668329e-09, ...,\n",
       "          1.85848748e-09,   6.83887556e-05,   6.26146502e-05],\n",
       "       ..., \n",
       "       [  1.03693338e-07,   9.87976478e-07,   2.62871424e-07, ...,\n",
       "          2.81841636e-07,   2.45729025e-04,   2.64674600e-05],\n",
       "       [  5.78437422e-08,   5.90050331e-06,   2.05155766e-06, ...,\n",
       "          3.41743487e-08,   5.03320643e-06,   5.74411126e-03],\n",
       "       [  3.09904920e-07,   1.53928904e-05,   3.38501413e-05, ...,\n",
       "          1.49684194e-08,   2.73647602e-04,   1.03317561e-05]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.predict_generator(train_gen, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try just adding another dense softmax with 2 outputs onto our model\n",
    "* This is where to read about building models https://keras.io/getting-started/sequential-model-guide/\n",
    "* The model is compiled now so can't be altered\n",
    "* But can take output of the model (which contains the graph of layers?) and add onto that\n",
    "* And then make new model from it's input and output objects\n",
    "* Don't forget to set which layers are trainable\n",
    "* NOTE: using an untrain layer, and trying to fine-tune train the whole model will result in bad training\n",
    "    * This is because the large and random params in the new layer will totally blow up the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_last = vgg.output\n",
    "x = Dense(2, activation='softmax')(vgg_last)\n",
    "v1_vgg = Model(inputs=vgg.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Models (setting their learning processes)\n",
    "* Compile is a important call - it configures it's learning process\n",
    "    * https://keras.io/models/model/\n",
    "* Where much of the model specifics are set\n",
    "    * Optimizer\n",
    "        * Learning rates etc. invovled in that optimizer\n",
    "    * Model metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "v1_vgg.compile(loss='categorical_crossentropy', # set the loss to optimize\n",
    "               optimizer=sgd, # set optimization method\n",
    "               metrics=['accuracy'] # sets metric to report (not used in computations though)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 138,359,546.0\n",
      "Trainable params: 2,002.0\n",
      "Non-trainable params: 138,357,544.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "v1_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "155/186 [========================>.....] - ETA: 51s - loss: 0.6572 - acc: 0.6070"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-6d69d91635d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m v1_vgg_hist = v1_vgg.fit_generator(train_gen, train_gen.nb_batches, epochs=1,\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_data=val_gen, validation_steps=val_gen.nb_batches)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/basev1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: it's not smart about how big an epoch is\n",
    "# you MUST specify the number of batches that will cover all samples\n",
    "# that is `samples/batch_size` (annoying it can't do this by default...?)\n",
    "v1_vgg_hist = v1_vgg.fit_generator(train_gen, train_gen.nb_batches, epochs=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_gen, validation_steps=val_gen.nb_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the history of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.68293714209606771,\n",
       "  0.66511032455845887,\n",
       "  0.67317484240782888,\n",
       "  0.67062816180680929,\n",
       "  0.66171474519528839,\n",
       "  0.67826822243238749,\n",
       "  0.6666666413893042,\n",
       "  0.67529707205922984,\n",
       "  0.6502546479827479,\n",
       "  0.66680811894567393],\n",
       " 'loss': [0.68619533275303091,\n",
       "  0.686047469314776,\n",
       "  0.68623615252344228,\n",
       "  0.68594373841034739,\n",
       "  0.68630401711714895,\n",
       "  0.68577568154585988,\n",
       "  0.68573595944915555,\n",
       "  0.68676247408515534,\n",
       "  0.68677550867984172,\n",
       "  0.68600516570241832],\n",
       " 'val_acc': [0.66499998724460607,\n",
       "  0.66949995291233066,\n",
       "  0.66099996101856229,\n",
       "  0.67299996483325963,\n",
       "  0.66899997329711913,\n",
       "  0.66199997210502626,\n",
       "  0.67799997901916509,\n",
       "  0.67699997031688686,\n",
       "  0.66799997425079349,\n",
       "  0.66849998021125789],\n",
       " 'val_loss': [0.68648747551441192,\n",
       "  0.68668375134468074,\n",
       "  0.68701954841613766,\n",
       "  0.68662157070636753,\n",
       "  0.68675719702243809,\n",
       "  0.68662935400009151,\n",
       "  0.68622168576717379,\n",
       "  0.68645757913589478,\n",
       "  0.68677108228206629,\n",
       "  0.68666307485103606]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_vgg_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b2d0379b0>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wOdFJgBMNyrpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b2f68c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(v1_vgg_hist.epoch, v1_vgg_hist.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "* Should make note of final scores on val set - use `model.evaluation_generator()`\n",
    "* Should view...\n",
    "    * Random correct sample\n",
    "    * Random incorrect sample\n",
    "    * Most right pred images in each class\n",
    "    * Most wrong pred images in each class\n",
    "    * Most unsure pred images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,label\r\n",
      "1,0.5\r\n",
      "2,0.5\r\n",
      "3,0.5\r\n",
      "4,0.5\r\n",
      "5,0.5\r\n",
      "6,0.5\r\n",
      "7,0.5\r\n",
      "8,0.5\r\n",
      "9,0.5\r\n"
     ]
    }
   ],
   "source": [
    "!head {DATA_PATH}sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
